{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9195c4b-e592-475d-a1d2-e493247de760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Veri başarıyla yüklendi.\n",
      "\n",
      "✅ 'Notr' etiketleri kaldırıldı. Kalan veri satır sayısı: 31873\n",
      "\n",
      "✅ Temel metin temizliği yapıldı.\n",
      "\n",
      "İşlenmiş Veri Kontrolü (İlk 5 Satır):\n",
      "                                                text  \\\n",
      "0   ısrarla korkutmayı başarıyor. sanki korku çok...   \n",
      "1  i phone 5 ten sonra gene 4'' ekranı tercih ett...   \n",
      "2  arkadaşımın ofisinde gördüm,  tavsiyesi üzerin...   \n",
      "3   cok sıkıcı bir film birkaç oyuncunun iyi perf...   \n",
      "4  ürün çok kısa bir sürede geldi ve çok kullanış...   \n",
      "\n",
      "                                          text_temiz     label  \n",
      "0  ısrarla korkutmayı başarıyor sanki korku çok u...  Positive  \n",
      "1  i phone ten sonra gene ekranı tercih ettim tel...  Positive  \n",
      "2  arkadaşımın ofisinde gördüm tavsiyesi üzerine ...  Positive  \n",
      "3  cok sıkıcı bir film birkaç oyuncunun iyi perfo...  Negative  \n",
      "4  ürün çok kısa bir sürede geldi ve çok kullanış...  Positive  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 1. VERİ YÜKLEME\n",
    "# ARTIK TAM YOL KULLANIYORUZ, GÖRELİ YOL SORUNUNU AŞTIK.\n",
    "# r'' kullanmak, Windows yolundaki \\ işaretlerini doğru okumasını sağlar.\n",
    "VERI_YOLU = r\"C:\\Users\\ozlem\\OneDrive\\Belgeler\\GitHub\\-twitter-sentiment-analizi\\data\\test.csv\" \n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(VERI_YOLU)\n",
    "    print(\"✅ Veri başarıyla yüklendi.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"HATA: Dosya '{VERI_YOLU}' yolunda hala bulunamıyor. Lütfen yolu manuel kontrol edin.\")\n",
    "    raise # Hata durumunda kodu tamamen durdururuz.\n",
    "\n",
    "# 2. TEMEL TEMİZLİK VE FİLTRELEME\n",
    "# Sadece text (tweet metni) ve label (etiket) sütunlarını alıyoruz.\n",
    "df = df[['text', 'label']] \n",
    "\n",
    "# 'Notr' etiketlerini kaldırıyoruz. Sadece 'Positive' ve 'Negative' kalacak.\n",
    "df = df[df['label'] != 'Notr'].reset_index(drop=True)\n",
    "print(f\"\\n✅ 'Notr' etiketleri kaldırıldı. Kalan veri satır sayısı: {len(df)}\")\n",
    "\n",
    "\n",
    "# 3. METİN TEMİZLİK FONKSİYONU\n",
    "def temel_metin_temizligi(text):\n",
    "    text = str(text)\n",
    "    # URL'leri, kullanıcı adlarını, noktalama ve sayıları temizler\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+|@\\w+|[^\\w\\s]|\\d+', '', text, flags=re.MULTILINE)\n",
    "    # Birden fazla boşluğu tek boşluğa indirme ve küçük harfe çevirme\n",
    "    text = re.sub(r'\\s+', ' ', text).strip().lower()\n",
    "    return text\n",
    "\n",
    "# Temizleme işlemini uygulama\n",
    "df['text_temiz'] = df['text'].apply(temel_metin_temizligi)\n",
    "\n",
    "print(\"\\n✅ Temel metin temizliği yapıldı.\")\n",
    "print(\"\\nİşlenmiş Veri Kontrolü (İlk 5 Satır):\")\n",
    "print(df[['text', 'text_temiz', 'label']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68186dc3-faff-4f3e-a1f3-39f2c1882063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ozlem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# Türkçe tokenization için gereken kaynak\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "193699e5-d278-45b0-88fe-2ea61229be5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ozlem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Stopwords ve Tokenization tamamlandı.\n",
      "\n",
      "İşlenmiş Veri Kontrolü:\n",
      "                                                text  \\\n",
      "0   ısrarla korkutmayı başarıyor. sanki korku çok...   \n",
      "1  i phone 5 ten sonra gene 4'' ekranı tercih ett...   \n",
      "2  arkadaşımın ofisinde gördüm,  tavsiyesi üzerin...   \n",
      "3   cok sıkıcı bir film birkaç oyuncunun iyi perf...   \n",
      "4  ürün çok kısa bir sürede geldi ve çok kullanış...   \n",
      "\n",
      "                                          text_temiz  \n",
      "0  ısrarla korkutmayı başarıyor sanki korku uzun ...  \n",
      "1  phone ten sonra gene ekranı tercih ettim telef...  \n",
      "2  arkadaşımın ofisinde gördüm tavsiyesi üzerine ...  \n",
      "3  cok sıkıcı bir film oyuncunun iyi performansı ...  \n",
      "4  ürün kısa bir sürede geldi kullanışlı arabamız...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# NLTK kütüphanesini kullanmak için gerekli Türkçe dosyasını indirme (ilk çalıştırmada gereklidir)\n",
    "try:\n",
    "    # Bu adımı daha önce yapmıştık, ama emin olmak için burada kalsın.\n",
    "    nltk.download('punkt')\n",
    "except:\n",
    "    print(\"NLTK 'punkt' zaten yüklü.\")\n",
    "\n",
    "# --- 1. Türkçe Stopwords Listesi ---\n",
    "turkce_stopwords = set([\n",
    "    \"a\", \"acaba\", \"ama\", \"aslında\", \"az\", \"bazı\", \"belki\", \"biri\", \"birkaç\", \"birşey\", \"biz\", \"böyle\", \n",
    "    \"bunu\", \"çünkü\", \"çok\", \"çoğu\", \"de\", \"da\", \"daha\", \"diye\", \"eğer\", \"fakat\", \"falan\", \"filan\", \n",
    "    \"gibi\", \"hâlâ\", \"hangi\", \"hemen\", \"her\", \"herkes\", \"hiç\", \"için\", \"ile\", \"ise\", \"ya\", \"ve\", \n",
    "    \"kim\", \"mı\", \"mı\", \"mu\", \"mü\", \"nasıl\", \"ne\", \"neden\", \"nedense\", \"nerde\", \"nereye\", \"niye\", \n",
    "    \"o\", \"oysa\", \"oysaki\", \"öbürü\", \"ön\", \"şayet\", \"şey\", \"şu\", \"tüm\", \"tek\", \"üzere\", \"yoksa\",\n",
    "    \"bu\", \"şu\", \"ki\", \"mi\", \"mı\", \"mu\", \"mü\", \"ne\", \"niye\", \"hangi\", \"kime\", \"kimden\"\n",
    "])\n",
    "\n",
    "# --- 2. Stopwords Kaldırma ve Tokenization Fonksiyonu ---\n",
    "def stopword_ve_tokenize(text):\n",
    "    tokens = word_tokenize(text, language='turkish')\n",
    "    filtered_tokens = [word for word in tokens if word not in turkce_stopwords and len(word) > 1]\n",
    "    return \" \".join(filtered_tokens) \n",
    "\n",
    "# Uygulama\n",
    "df['text_temiz'] = df['text_temiz'].apply(stopword_ve_tokenize)\n",
    "\n",
    "print(\"✅ Stopwords ve Tokenization tamamlandı.\")\n",
    "print(\"\\nİşlenmiş Veri Kontrolü:\")\n",
    "print(df[['text', 'text_temiz']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02b1d3e1-8066-471d-b6ad-45811f0550d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Etiketler Sayısallaştırıldı. (0: Negative, 1: Positive)\n",
      "\n",
      "✅ Veri, eğitim ve test setlerine ayrıldı.\n",
      "Eğitim Seti Büyüklüğü: 25498 satır\n",
      "Test Seti Büyüklüğü: 6375 satır\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# --- 1. Etiketleri Sayısallaştırma (Negative=0, Positive=1) ---\n",
    "le = LabelEncoder()\n",
    "# Etiketleri dönüştürme: Negative -> 0, Positive -> 1\n",
    "df['label_sayisal'] = le.fit_transform(df['label']) \n",
    "print(\"✅ Etiketler Sayısallaştırıldı. (0: Negative, 1: Positive)\")\n",
    "\n",
    "# --- 2. Veriyi Bağımlı ve Bağımsız Değişkenlere Ayırma ---\n",
    "# X: Temizlenmiş Metin (bağımsız değişken)\n",
    "# y: Sayısallaştırılmış Etiket (bağımlı değişken)\n",
    "X = df['text_temiz']\n",
    "y = df['label_sayisal']\n",
    "\n",
    "# --- 3. Eğitim ve Test Setlerine Ayırma (Model Eğitimi için) ---\n",
    "# Verinin %80'i eğitim, %20'si test için kullanılacak.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "print(f\"\\n✅ Veri, eğitim ve test setlerine ayrıldı.\")\n",
    "print(f\"Eğitim Seti Büyüklüğü: {len(X_train)} satır\")\n",
    "print(f\"Test Seti Büyüklüğü: {len(X_test)} satır\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cececaab-fe79-4bfd-8a1f-d503369db5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TF-IDF Vektörleştirme tamamlandı.\n",
      "Eğitim Matrisi Şekli (Satır, Özellik/Kelime Sayısı): (25498, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF Vektörleyici Oluşturma\n",
    "# max_features: En sık kullanılan 5000 kelimeyi alıyoruz.\n",
    "# ngram_range: Tekli kelimelerin yanı sıra ikili kelime gruplarını da dahil ediyoruz.\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "\n",
    "# Eğitim verisi üzerinde eğitme ve DÖNÜŞTÜRME\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Test verisini dönüştürme (Sadece transform, fit değil!)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "print(\"✅ TF-IDF Vektörleştirme tamamlandı.\")\n",
    "print(f\"Eğitim Matrisi Şekli (Satır, Özellik/Kelime Sayısı): {X_train_vec.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f1d106-a869-4e0a-aeaf-d37e636764f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
